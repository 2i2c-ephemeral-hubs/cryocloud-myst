{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fd4844a-aee8-4a9c-b22a-02688a8067f9",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Part 1: Introduction to the `earthaccess` python library\n",
    "\n",
    "## Tutorial Overview\n",
    "\n",
    "This tutorial is designed for the \"[Cloud Computing and Open-Source Scientific Software for Cryosphere Communities](https://agu.confex.com/agu/fm23/meetingapp.cgi/Session/193477)\" Learning Workshop at the 2023 AGU Fall Meeting.\n",
    "\n",
    "This notebook demonstrates how to search for, access, and work with a cloud-hosted NASA dataset using the `earthaccess` package. Data in the \"NASA Earthdata Cloud\" are stored in Amazon Web Services (AWS) Simple Storage Service (S3) Buckets. **Direct Access** is an efficient way to work with data stored in an S3 Bucket using an Amazon Compute Cloud (EC2) instance. Cloud-hosted granules can be opened and loaded into memory without the need to download them first. This allows you take advantage of the scalability and power of cloud computing. \n",
    "\n",
    "We use `earthaccess`, a package developed by Luis Lopez (NSIDC developer) to allow easy search of the NASA Common Metadata Repository (CMR) and download of NASA data collections. It can be used for programmatic search and access for both _DAAC-hosted_ and _cloud-hosted_ data. It manages authenticating using Earthdata Login credentials which are then used to obtain the S3 tokens that are needed for S3 direct access. `earthaccess` can be used to find and access both DAAC-hosted and cloud-hosted data in just **three** lines of code. See [https://github.com/nsidc/earthaccess](https://github.com/nsidc/earthaccess).\n",
    "\n",
    "As an example data collection, we use ICESat-2 Land Ice Height (ATL06) granules over the Juneau Icefield, AK, for March and April 2020. ICESat-2 data granules, including ATL06, are stored in HDF5 format. We demonstrate how to open an HDF5 granule and access data variables using `xarray`. Land Ice Heights are then plotted using `hvplot`. \n",
    "\n",
    "![ExamplePlotusingTutorialData](./images/atl06_example_plot.png)\n",
    "> ATL06 Land Ice Heights for the margin of the Juneau Ice Field\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "In this tutorial you will learn:  \n",
    "1. how to use `earthaccess` to search for (ICESat-2) data using spatial and temporal filters and explore the search results;  \n",
    "2. how to open data granules using direct access to the appropriate S3 bucket;  \n",
    "3. how to load an HDF5 group into an `xarray.Dataset`;  \n",
    "4. how visualize the land ice heights using `hvplot`.  \n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "The workflow described in this tutorial forms the initial steps of an _Analysis in Place_ workflow that would be run on a AWS cloud compute resource.  You will need:\n",
    "\n",
    "1. a JupyterHub, such as CryoHub, or AWS EC2 instance in the us-west-2 region.\n",
    "3. a NASA Earthdata Login.  If you need to register for an Earthdata Login see the [Getting an Earthdata Login](https://icesat-2-2023.hackweek.io/preliminary/checklist/earthdata.html#getting-an-earthdata-login) section of the ICESat-2 Hackweek 2023 Jupyter Book.\n",
    "4. A `.netrc` file, that contains your Earthdata Login credentials, in your home directory. See [Configure Programmatic Access to NASA Servers](https://icesat-2-2023.hackweek.io/preliminary/checklist/earthdata.html#configure-programmatic-access-to-nasa-servers) to create a `.netrc` file.\n",
    "\n",
    "## Credits\n",
    "\n",
    "This notebook is based on an [NSIDC Data Tutorial](https://github.com/nsidc/NSIDC-Data-Tutorials) originally created by Luis Lopez, NSIDC, and Mikala Beig, NSIDC, modified by Andy Barrett, NSIDC, Jennifer Roebuck, NSIDC, Amy Steiker, NSIDC, and Jessica Scheick, Univ. of New Hampshire."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb08333-e7f5-4c78-98c4-949313b6d481",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Computing Environment\n",
    "\n",
    "The tutorial uses `python` and requires the following packages:\n",
    "- `earthaccess`, which enables Earthdata Login authentication and retrieves AWS credentials; enables collection and granule searches; and S3 access;\n",
    "- `xarray`, used to load N-dimensional data with labeled axes;\n",
    "- `hvplot`, used to visualize land ice height data.\n",
    "\n",
    "We are going to import the whole `earthaccess` package.\n",
    "\n",
    "We will also import the whole `xarray` package but use a standard short name `xr`, using the `import <package> as <short_name>` syntax.  We could use anything for a short name but `xr` is an accepted standard that most `xarray` users are familiar with.\n",
    "\n",
    "[`xarray`](https://docs.xarray.dev/en/stable/) is a powerful library for working with multi-dimensional data using labeled indices (analogous to Pandas for tabular data). It is leverages numpy, pandas, matplotlib and dask to build Dataset and DataArray objects with built-in methods to subset, analyze, interpolate, and plot multi-dimensional data. It makes working with multi-dimensional data cubes efficient and fun. A few great tutorials for learning Xarray are [here](https://nasa-openscapes.github.io/2021-Cloud-Hackathon/tutorials/03_Xarray.html) and [here](https://tutorial.xarray.dev/intro.html).\n",
    "\n",
    "We only need the `xarray` module from `hvplot` so we import that using the `import <package>.<module>` syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c965d59f-5e6b-45cf-a63b-694c94e95d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For searching and accessing NASA data\n",
    "import earthaccess\n",
    "\n",
    "# For reading data, analysis and plotting\n",
    "import xarray as xr\n",
    "import hvplot.xarray\n",
    "\n",
    "import pprint  # For nice printing of python objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf13e8ff-a256-4108-bc6c-5e20fc8f321c",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Authenticate\n",
    "\n",
    "The first step is to get the correct authentication to access _cloud-hosted_ ICESat-2 data.  This is all done through Earthdata Login.  The `login` method also gets the correct AWS credentials.\n",
    "\n",
    "Login requires your Earthdata Login username and password. The `login` method will automatically search for these credentials as environment variables or in a `.netrc` file, and if those aren't available it will prompt you to enter your username and password. We use the prompt strategy here. A `.netrc` file is a text file located in our home directory that contains login information for remote machines.  If you don't have a `.netrc` file, `login` will create one for you if you use `persist=True`.\n",
    "\n",
    "```\n",
    "earthaccess.login(strategy='interactive', persist=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd332b9e-0aca-4c47-9d4e-8d92a48b2e42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "auth = earthaccess.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd4d20c-71bb-4a49-85c1-d43e7e9eeb3e",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Search for ICESat-2 Collections\n",
    "\n",
    "`earthaccess` leverages the Common Metadata Repository (CMR) API to search for collections and granules.  [Earthdata Search](https://search.earthdata.nasa.gov/search) also uses the CMR API.\n",
    "\n",
    "We can use the `search_datasets` method to search for ICESat-2 collections by setting `keyword=\"ICESat-2\"`.  The argument passed to `keyword` can be any string and can include wildcard characters `?` or `*`.\n",
    "\n",
    "```{note}\n",
    "To see a full list of search parameters you can type `earthaccess.search_datasets?`.  Using `?` after a python object displays the `docstring` for that object.\n",
    "```\n",
    "\n",
    "A count of the number of data collections (Datasets) found is given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6131f7-0f3c-4227-9301-618f364dcec6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = earthaccess.search_datasets(\n",
    "            keyword=\"ICESat-2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd55ecf-6245-4caa-bd7e-903374086827",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "In this case, there are 89 datasets that have the keyword ICESat-2.  \n",
    "\n",
    "`search_datasets` returns a python list of `DataCollection` objects.  We can view metadata for each collection in long form by passing a `DataCollection` object to print or as a summary using the `summary` method for the `DataCollection` object.  Here, I use the `pprint` function to _Pretty Print_ each object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54b13d9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for collection in query[:10]:\n",
    "    pprint.pprint(collection.summary(), sort_dicts=True, indent=4)\n",
    "    print('')  # Add a space between collections for readability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e7fbf6-44f3-46df-99c6-5a138c2e7b11",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "For each collection, `summary` returns a subset of fields from the collection metadata and Unified Metadata Model (UMM) entry.\n",
    "\n",
    "- `concept-id` is an unique identifier for the collection that is composed of a alphanumeric code and the provider-id for the DAAC.\n",
    "- `short-name` is the name of the dataset that appears on the dataset set landing page. For ICESat-2, `ShortNames` are generally how different products are referred to.\n",
    "- `version` is the version of each collection.\n",
    "- `file-type` gives information about the file format of the collection files.\n",
    "- `get-data` is a collection of URLs that can be used to access data, dataset landing pages, and tools.  \n",
    "\n",
    "For _cloud-hosted_ data, there is additional information about the location of the S3 bucket that holds the data and where to get credentials to access the S3 buckets.  In general, you don't need to worry about this information because `earthaccess` handles S3 credentials for you.  Nevertheless it may be useful for troubleshooting. \n",
    "\n",
    "```{note}\n",
    "In Python, all data are represented by _objects_.  These _objects_ contain both data and methods (think functions) that operate on the data.  `earthaccess` includes  `DataCollection` and `DataGranule` objects that contain data about collections and granules returned by `search_datasets` and `search_data` respectively.  If you are familiar with Python, you will see that the data in each `DataCollection` object is organized as a hierarchy of python dictionaries, lists and strings.  So if you know the dictionary key for the metadata entry you want you can get that metadata using standard dictionary methods.  For example, to get the dataset short name from the example below, you could just use `collection['meta']['concept-id']`.  However, in this example the `concept-id' method for the DataCollection object returns the same information.  Take a look at https://github.com/nsidc/earthaccess/blob/main/earthaccess/results.py#L80 to see how this is done.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f2d885-ea84-44fe-b2d8-cf139562fc70",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "For the ICESat-2 search results the concept-id is `NSIDC_ECS` or `NSIDC_CPRD`. `NSIDC_ECS` is for collections archived at the NSIDC DAAC and `NSIDC_CPRD` is for the _cloud-hosted_ collections. \n",
    "\n",
    "For ICESat-2 `short-name` refers to the following products.  \n",
    "\n",
    "| ShortName | Product Description |\n",
    "|:-----------:|:---------------------|\n",
    "| ATL03 | ATLAS/ICESat-2 L2A Global Geolocated Photon Data |\n",
    "| ATL06 | ATLAS/ICESat-2 L3A Land Ice Height |\n",
    "| ATL07 | ATLAS/ICESat-2 L3A Sea Ice Height |\n",
    "| ATL08 | ATLAS/ICESat-2 L3A Land and Vegetation Height |\n",
    "| ATL09 | ATLAS/ICESat-2 L3A Calibrated Backscatter Profiles and Atmospheric Layer Characteristics |\n",
    "| ATL10 | ATLAS/ICESat-2 L3A Sea Ice Freeboard |\n",
    "| ATL11 | ATLAS/ICESat-2 L3B Slope-Corrected Land Ice Height Time Series |\n",
    "| ATL12 | ATLAS/ICESat-2 L3A Ocean Surface Height |\n",
    "| ATL13 | ATLAS/ICESat-2 L3A Along Track Inland Surface Water Data |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5329bbf6-4530-4dfd-88dd-153a1fb5d862",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Search for cloud-hosted data\n",
    "\n",
    "If you only want to search for data in the cloud, you can set `cloud_hosted=True`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322d78c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Query = earthaccess.search_datasets(\n",
    "    keyword = 'ICESat-2',\n",
    "    cloud_hosted = True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904e589e-4524-4aaa-9c91-8f464c0ccb96",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Search a data set using spatial and temporal filters \n",
    "\n",
    "Once, you have identified the dataset you want to work with, you can use the `search_data` method to search a data set with spatial and temporal filters. As an example, we'll search for ATL06 granules over the Juneau Icefield, AK, for March and April 2020.\n",
    "\n",
    "Either `concept-id` or `short-name` can be used to search for granules from a particular dataset.  If you use `short-name` you also need to set `version`.  If you use `concept-id`, this is all that is required because `concept-id` is unique. \n",
    "\n",
    "The temporal range is identified with standard date strings. Latitude-longitude corners of a bounding box are specified as lower left, upper right.  Polygons and points, as well as shapefiles can also be specified.\n",
    "\n",
    "This will display the number of granules that match our search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fba5c34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = earthaccess.search_data(\n",
    "    short_name = 'ATL06',\n",
    "    version = '006',\n",
    "    cloud_hosted = True,\n",
    "    bounding_box = (-134.7,58.9,-133.9,59.2),\n",
    "    temporal = ('2020-03-01','2020-04-30'),\n",
    "    count = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89147064-a839-4900-beea-9cc5c228ce04",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "We'll get metadata for these 4 granules and display it.  The rendered metadata shows a download link, granule size and two images of the data.\n",
    "\n",
    "The download link is `https` and can be used download the granule to your local machine.  This is similar to downloading _DAAC-hosted_ data but in this case the data are coming from the Earthdata Cloud.  For NASA data in the Earthdata Cloud, there is no charge to the user for egress from AWS Cloud servers.  This is not the case for other data in the cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04370d3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "[display(r) for r in results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944dc900-02c7-43fa-bef8-12d91b566195",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Use Direct-Access to open, load and display data stored on S3\n",
    "\n",
    "Direct-access to data from an S3 bucket is a two step process.  First, the files are opened using the `open` method.  This first step creates a Python file-like object that is used to load the data in the second step.  \n",
    "\n",
    "Authentication is required for this step.  The `auth` object created at the start of the notebook is used to provide Earthdata Login authentication and AWS credentials \"_behind-the-scenes_\". These credentials expire after one hour so the `auth` object must be executed within that time window prior to these next steps. \n",
    "\n",
    "```{note}\n",
    "The `open` step to create a file-like object is required because AWS S3, and other cloud storage systems, use  object storage but most HDF5 libraries work with POSIX-compliant file systems.  POSIX stands for Portable Operating System Interface for Unix and is a set of guidelines that include how to interact with files and file systems.  Linux, Unix, MacOS (which is Unix-like), and Windows are POSIX-compliant. Critically, POSIX-compliant systems allows blocks of bytes or individual bytes to be read from a file. With object storage the whole file has to be read. To get around this limitation, an intermediary is used, in this case `s3fs`.  This intermediary creates a local POSIX-compliant virtual file system.  S3 objects are loaded into this virtual file system so they can be accessed using POSIX-style file functions.\n",
    "```\n",
    "\n",
    "In this example, data are loaded into an `xarray.Dataset`.  Data could be read into `numpy` arrays or a `pandas.Dataframe`.  However, each granule would have to be read using a package that reads HDF5 granules such as `h5py`.  `xarray` does this all _under-the-hood_ in a single line but only for a single group in the HDF5 granule, in this case land ice heights for the gt1l beam*.\n",
    "\n",
    "*ICESat-2 measures photon returns from 3 beam pairs numbered 1, 2 and 3 that each consist of a left and a right beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11205bbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%time\n",
    "files = earthaccess.open(results)\n",
    "ds = xr.open_dataset(files[1], group='/gt1l/land_ice_segments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75881751",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a261535-692b-4e3a-af1f-e7316b7f9084",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "`hvplot` is an interactive plotting tool that is useful for exploring data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7386c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds['h_li'].hvplot(kind='scatter', s=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b656e1-9a42-4656-bd59-79256053f1c8",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Additional resources\n",
    "\n",
    "For general information about NSIDC DAAC data in the Earthdata Cloud: \n",
    "\n",
    "[FAQs About NSIDC DAAC's Earthdata Cloud Migration](https://nsidc.org/data/user-resources/help-center/faqs-about-nsidc-daacs-earthdata-cloud-migration)\n",
    "\n",
    "[NASA Earthdata Cloud Data Access Guide](https://nsidc.org/data/user-resources/help-center/nasa-earthdata-cloud-data-access-guide)\n",
    "\n",
    "Additional tutorials and How Tos:\n",
    "\n",
    "[NASA Earthdata Cloud Cookbook](https://nasa-openscapes.github.io/earthdata-cloud-cookbook/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
